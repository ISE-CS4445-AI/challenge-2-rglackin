{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTTUuzCyzRqG"
      },
      "source": [
        "# Challenge 2 - Data Preprocessing\n",
        "\n",
        "Welcome to challenge 2/7! You will be attempting two parts just like in challenge 1 last week i.e., the MCQ section and the code challenge section.\n",
        "\n",
        "Good luck! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROAjx16L5x5Y"
      },
      "source": [
        "## Section 1: Multiple Choice Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRALy2NEz0AS"
      },
      "source": [
        "### Q1. What is the primary goal of data preprocessing in machine learning? (1 point)\n",
        "**Options:**\n",
        "\n",
        "\n",
        "1.   To train the model directly.\n",
        "2.  To clean and prepare raw data for better model performance.\n",
        "3.  To increase the size of the dataset.\n",
        "4.  To avoid splitting the dataset into train and test sets.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFtBxSL40CG9",
        "outputId": "49e0c805-cd5f-4d4b-be20-2cf551c8bc86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q1: What is the primary goal of data preprocessing in machine learning?\n",
            "Answer: To clean and prepare raw data for better model performance\n"
          ]
        }
      ],
      "source": [
        "def answer_q1():\n",
        "    \"\"\"\n",
        "    Q1: What is the primary goal of data preprocessing in machine learning?\n",
        "    \"\"\"\n",
        "\n",
        "    options = {\n",
        "        1: \"To train the model directly\",\n",
        "        2: \"To clean and prepare raw data for better model performance\",\n",
        "        3: \"To increase the size of the dataset\",\n",
        "        4: \"To avoid splitting the dataset into train and test sets\"\n",
        "    }\n",
        "\n",
        "    # TODO: return the correct option number\n",
        "    return options[2]\n",
        "\n",
        "print(f'Q1: What is the primary goal of data preprocessing in machine learning?\\nAnswer: {answer_q1()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9BRj9Qv0v9o"
      },
      "source": [
        "### Q2. Which of the following techniques is used to handle missing numerical data? (1 point)\n",
        "**Options:**\n",
        "1. One-hot encoding\n",
        "2. Imputing with mean, median, or mode\n",
        "3. Removing categorical columns\n",
        "4. Detecting outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uFto7n3ny7UA",
        "outputId": "2025056d-b701-4a72-89b4-7a459c2828b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q2: Which of the following techniques is used to handle missing numerical data?\n",
            "Answer: Imputing with mean, median, or mode\n"
          ]
        }
      ],
      "source": [
        "def answer_q2():\n",
        "    \"\"\"\n",
        "    Q2: Which of the following techniques is used to handle missing numerical data?\n",
        "    \"\"\"\n",
        "\n",
        "    options = {\n",
        "        1: \"One-hot encoding\",\n",
        "        2: \"Imputing with mean, median, or mode\",\n",
        "        3: \"Removing categorical columns\",\n",
        "        4: \"Detecting outliers\"\n",
        "    }\n",
        "\n",
        "    # TODO: return the correct option number\n",
        "    return options[2]\n",
        "\n",
        "print(f'Q2: Which of the following techniques is used to handle missing numerical data?\\nAnswer: {answer_q2()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAYgSO7Z2FTd"
      },
      "source": [
        "### Q3. What does one-hot encoding do? (1 point)\n",
        "**Options:**\n",
        "1. Replaces missing numerical values with zeros.\n",
        "2. Converts numerical columns into categorical columns.\n",
        "3. Converts categorical data into binary columns, one for each category.\n",
        "4. Detects and removes outliers from a dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5SAsK9C2Z8R",
        "outputId": "edd20aeb-bf1e-4fcf-fd84-17c8e39a92a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q3: What does one-hot encoding do?\n",
            "Answer: Converts categorical data into binary columns, one for each category\n"
          ]
        }
      ],
      "source": [
        "def answer_q3():\n",
        "    \"\"\"\n",
        "    Q3: What does one-hot encoding do?\n",
        "    \"\"\"\n",
        "\n",
        "    options = {\n",
        "        1: \"Replaces missing numerical values with zeros\",\n",
        "        2: \"Converts numerical columns into categorical columns\",\n",
        "        3: \"Converts categorical data into binary columns, one for each category\",\n",
        "        4: \"Detects and removes outliers from a dataset\"\n",
        "    }\n",
        "\n",
        "    # TODO: return the correct option number\n",
        "    return options[3]\n",
        "\n",
        "print(f'Q3: What does one-hot encoding do?\\nAnswer: {answer_q3()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shVhsUec2zS4"
      },
      "source": [
        "### Q4.  Which method can be used to detect outliers in a dataset? (1 point)\n",
        "**Options:**\n",
        "1.  Box plot visualization\n",
        "2.  Calculating the correlation matrix\n",
        "3.  Applying one-hot encoding\n",
        "4.  Normalizing the data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS7K8DqW4Hpr",
        "outputId": "56665bdf-af18-4045-b17a-0489035b20cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q4: Which method can be used to detect outliers in a dataset?\n",
            "Answer: Box plot visualization\n"
          ]
        }
      ],
      "source": [
        "def answer_q4():\n",
        "    \"\"\"\n",
        "    Q4: Which method can be used to detect outliers in a dataset?\n",
        "    \"\"\"\n",
        "\n",
        "    options = {\n",
        "        1: \"Box plot visualization\",\n",
        "        2: \"Calculating the correlation matrix\",\n",
        "        3: \"Applying one-hot encoding\",\n",
        "        4: \"Normalizing the data\"\n",
        "    }\n",
        "\n",
        "    # TODO: return the correct option number\n",
        "    return options[1]\n",
        "\n",
        "print(f'Q4: Which method can be used to detect outliers in a dataset?\\nAnswer: {answer_q4()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UizfbmVn46j1"
      },
      "source": [
        "### Q5. You are working with a dataset containing a timestamp column (purchase_date). Which of the following feature engineering techniques would help capture temporal patterns effectively for predictive modeling? (2 points)\n",
        "**Options:**\n",
        "1. Extracting the day of the week, month, and hour from purchase_date.\n",
        "2. Converting the purchase_date to a UNIX timestamp and normalizing it.\n",
        "3. Creating features like \"days since last purchase\" for each customer.\n",
        "4. All of the above.\n",
        "5. None of the above.\n",
        "\n",
        "For this question alone, justify your answer briefly by replacing the string value of `why` with your explanation in addition to returning the correct option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAeIC4Us5Lst",
        "outputId": "f98a3b83-386b-419e-f25a-a438df3c986f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Q5: You are working with a dataset containing a timestamp column (purchase_date).Which of the following feature engineering techniques would help capture temporal patterns effectively for predictive modeling?\n",
            "Answer: ('Extracting the day of the week, month, and hour from purchase_date', 'Having day of week, month and hour will allow us to see temporal patterns such as busy times of day on certain days of the week, or busy days in certain months. Predictions can then be made using the business of certain times.')\n"
          ]
        }
      ],
      "source": [
        "def answer_q5():\n",
        "    \"\"\"\n",
        "    Q5: You are working with a dataset containing a timestamp column (purchase_date).\n",
        "    Which of the following feature engineering techniques would help capture temporal\n",
        "    patterns effectively for predictive modeling?\n",
        "    \"\"\"\n",
        "\n",
        "    options = {\n",
        "        1: \"Extracting the day of the week, month, and hour from purchase_date\",\n",
        "        2: \"Converting the purchase_date to a UNIX timestamp and normalizing it\",\n",
        "        3: \"Creating features like 'days since last purchase' for each customer\",\n",
        "        4: \"All of the above\",\n",
        "        5: \"None of the above\"\n",
        "    }\n",
        "\n",
        "    # TODO: return the correct option number\n",
        "    # Include your justification below in a line or two\n",
        "    why = \"Having day of week, month and hour will allow us to see temporal patterns such as busy times of day on certain days of the week, or busy days in certain months. Predictions can then be made using the business of certain times.\"\n",
        "    return options[1], why\n",
        "\n",
        "print(f' Q5: You are working with a dataset containing a timestamp column (purchase_date).Which of the following feature engineering techniques would help capture temporal patterns effectively for predictive modeling?\\nAnswer: {answer_q5()}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2tw01le53fJ"
      },
      "source": [
        "---\n",
        "\n",
        "## Section 2: Code challenge (4 points)\n",
        "\n",
        "To pass this section of the weekly challenge, uncomment/fill in code where necessary (marked with a 'TODO:' comment).\n",
        "\n",
        "### 1. House keeping and imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Cm3MG7iySn15"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hry_mfD2St36"
      },
      "source": [
        "### 2. Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6L9tk63SxuZ"
      },
      "outputs": [],
      "source": [
        "def load_data(file_path):\n",
        "    # TODO: Load the dataset\n",
        "    # data = ...\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "f1hXAB3jUgKx",
        "outputId": "741033c1-54fa-4302-e476-e7ca0ec1ecc4"
      },
      "outputs": [],
      "source": [
        "data = load_data(\"titanic.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Explore dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO Display the first few rows of the dataset\n",
        "# data...."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "id": "LIx_rzFPcVf7",
        "outputId": "49126114-f294-41c4-cf2f-35a1686dcad3"
      },
      "outputs": [],
      "source": [
        "# TODO: Display the dataset's information\n",
        "# data...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fill in the code below to return a dataframe that has the total number of missing (null) values per column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def number_missing_values(data):\n",
        "    # TODO: Return the number of missing values in each column\n",
        "    # return data....\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ww-oRhCNS0_6"
      },
      "source": [
        "### 4. Handle Missing Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_final(df):\n",
        "    \"\"\"\n",
        "    1) fix missing data (Age, Embarked, etc.)\n",
        "    2) drop unneeded columns (Cabin, Ticket, Name, PassengerId)\n",
        "    3) encode 'Sex', 'Embarked'\n",
        "    4) create at least 1 new feature\n",
        "    5) ensure no NaN in final numeric columns\n",
        "    \n",
        "    returns df_processed with no missing values in essential columns\n",
        "    \"\"\"\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # TODO: handle missing Age (fill with median?), handle missing Embarked, etc.\n",
        "    # median_age = ...\n",
        "    # df_processed['Age'] = ...\n",
        "    \n",
        "    # TODO: drop columns\n",
        "    # drop_cols = [...]\n",
        "    # df_processed = ...\n",
        "    \n",
        "    # TODO: binary encode 'Sex'\n",
        "    # df_processed['Sex'] = ...\n",
        "    \n",
        "    # fill the 2 empty values with the most common value 'S'\n",
        "    df_processed['Embarked'] = df_processed['Embarked'].fillna('S')\n",
        "    # TODO: one-hot encode 'Embarked'\n",
        "    # df_processed = ...\n",
        "    \n",
        "    # TODO: create at least 1 new feature\n",
        "    # e.g. FamilySize = SibSp + Parch + 1\n",
        "    # df_processed['FamilySize'] = ...\n",
        "    \n",
        "    # final step: drop or fill any remaining missing\n",
        "    df_processed.dropna(inplace=True)\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "\n",
        "# try with train data\n",
        "df_train = load_data('titanic.csv')\n",
        "df_processed = preprocess_final(df_train)\n",
        "print(\"After preprocess_final():\")\n",
        "print(\"Columns:\", df_processed.columns)\n",
        "print(\"Missing Values:\", df_processed.isnull().sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display the dataset's common statistical information after preprocessing\n",
        "df_processed.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FaVV4aDxYEUH"
      },
      "source": [
        "### 5. Looking at outliers\n",
        "It is often useful to look at data for outliers. The following function plots histograms for all the numerical data in a dataframe. This will give an idea of the general distribution of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Ks_Nl1DZKtQ"
      },
      "outputs": [],
      "source": [
        "def plot_histograms(df, bins=20):\n",
        "    \"\"\"\n",
        "    Plots histograms for all columns in a DataFrame.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): The input DataFrame.\n",
        "        bins (int, optional): Number of bins for the histograms. Defaults to 20.\n",
        "    \"\"\"\n",
        "    # Get numeric columns for histogram plotting\n",
        "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
        "\n",
        "    # Create subplots\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
        "    axes = axes.flatten()  # Flatten to iterate easily\n",
        "\n",
        "    # Iterate and plot histograms for each numeric column\n",
        "    for i, col in enumerate(numeric_columns):\n",
        "        axes[i].hist(df[col], bins=bins) # The bins argument should be an integer for number of bins, not column\n",
        "        axes[i].set_title(f'Histogram of {col}')\n",
        "        axes[i].set_xlabel(col)\n",
        "        axes[i].set_ylabel('Frequency')\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for j in range(i + 1, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot the histograms for the numerical features of the processed dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "Y28prbvuZ3WC",
        "outputId": "cc70225e-c8ea-450c-8db0-c2cf12982e8c"
      },
      "outputs": [],
      "source": [
        "plot_histograms(df_processed, bins=20)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A boxplot gives the general distribution of data in terms of quartile ranges and shows us outliers very clearly. Running the code cell below will show the box plot for the `Age` column grouped by the target column of `Survived`.  \n",
        "This shows us the median age of those who survived and didn't as well as certain data points that seem to be against the norm."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_processed.boxplot(column='Age', by='Survived', grid=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TODO: SHould the age outliers should be capped or removed from the dataset. Briefly justify."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your answer here..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kyJw8z1wTbBd"
      },
      "source": [
        "### 6. Training and testing (not graded)\n",
        "Now that we have our clean and pre-processed data, feel free to run a classifier of choice and see the accuracy you get. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdQ3FZ4dThWY"
      },
      "outputs": [],
      "source": [
        "def train_test(df):\n",
        "    \"\"\"\n",
        "    Trains a classifier model on the input DataFrame.\n",
        "    Returns:\n",
        "        float: Accuracy of the trained model.\n",
        "    \"\"\"\n",
        "    # Optional: \n",
        "    # train and test a classifier model on the pre-processed data\n",
        "    # X, y = df.drop('Survived', axis=1).values, df['Survived'].values\n",
        "    # ...\n",
        "\n",
        "    # return accuracy\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "VnXRdEERRxTD",
        "outputId": "8b7b4974-9038-4f61-a191-4a7bddecdfb1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# accuracy = train_test(df_processed)\n",
        "print(f\"Model Accuracy: {100 * accuracy:.2f}%\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
